{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Logistic Regression and Ensemble Methods***\n",
    "\n",
    "### 1. Logistic Regression\n",
    "Logistic regression is used for classification problems, where the goal is to predict a binary outcome given input data. Given a dataset:\n",
    "\n",
    "$$\n",
    "(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\n",
    "$$\n",
    "\n",
    "where $ y_i $ is either $ 0 $ or $ 1 $, we want to model the probability:\n",
    "\n",
    "$$\n",
    "P(y = 1 | x).\n",
    "$$\n",
    "\n",
    "To achieve this, we use the **sigmoid function**, which maps real numbers to the range $ (0,1) $, making it suitable for probability estimation:\n",
    "\n",
    "$$\n",
    "P(y = 1 | x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}.\n",
    "$$\n",
    "\n",
    "Similarly, for the probability of $ y = 0 $:\n",
    "\n",
    "$$\n",
    "P(y = 0 | x) = 1 - P(y = 1 | x).\n",
    "$$\n",
    "\n",
    "### 2. Multiclass Classification and Softmax Function\n",
    "For **multiclass classification**, logistic regression is extended using the **softmax function**. Suppose $ y $ can take multiple values: $ 1, 2, 3, \\dots, M $, then we define:\n",
    "\n",
    "$$\n",
    "P(y = j | x) = \\frac{e^{\\beta_j x}}{\\sum_{k=1}^{M} e^{\\beta_k x}}.\n",
    "$$\n",
    "\n",
    "Unlike the binary logistic function, softmax ensures that all class probabilities sum to 1.\n",
    "\n",
    "### 3. Statistical Mechanics and Ensembles\n",
    "The probabilistic formulation of logistic regression can be understood using ideas from **statistical mechanics**, particularly **Boltzmann’s work on ensembles**.\n",
    "\n",
    "- **Total $ N $ data points** are considered as an **ensemble**.\n",
    "- **Each data point is a system**, and it can exist in **one of M possible states** (labels).\n",
    "- The **macrostate** of the system is defined by how the $ N $ data points are distributed across the $ M $ labels.\n",
    "\n",
    "Now, the probability of a specific configuration $ (a_1, a_2, \\dots, a_M) $ occurring follows from the principle of maximum entropy and is given by:\n",
    "\n",
    "$$\n",
    "P(a_1, a_2, \\dots, a_M) = \\frac{W(a_1, a_2, \\dots, a_M)}{W_{\\text{total}}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ W(a_1, a_2, \\dots, a_M) $ is the number of ways to assign $ N $ data points into $ M $ states,\n",
    "- $ W_{\\text{total}} $ is the total number of possible assignments.\n",
    "\n",
    "Using the multinomial expression for $ W $:\n",
    "\n",
    "$$\n",
    "W(a_1, a_2, \\dots, a_M) = \\frac{N!}{a_1! a_2! \\dots a_M!}.\n",
    "$$\n",
    "\n",
    "Applying Stirling’s approximation $ \\ln N! \\approx N \\ln N - N $, we derive the most probable distribution by maximizing $ \\ln W $ subject to constraints.\n",
    "\n",
    "This leads to the probability of a system being in state $ j $:\n",
    "\n",
    "$$\n",
    "P_j = \\frac{W_j}{W_{\\text{total}}} = \\frac{e^{\\beta X_j}}{\\sum_{k=1}^{M} e^{\\beta X_k}}.\n",
    "$$\n",
    "\n",
    "Using combinatorial arguments, the number of ways to assign $ N $ data points across $ M $ states (labels) is given by:\n",
    "\n",
    "$$\n",
    "W(a_1, a_2, \\dots, a_M) = \\frac{N!}{a_1! a_2! \\dots a_M!}.\n",
    "$$\n",
    "\n",
    "Using **Lagrange multipliers** to enforce constraints (normalization and fixed average cost), we derive the probability:\n",
    "\n",
    "$$\n",
    "P(y = j) = \\frac{e^{-\\beta E_j}}{Z},\n",
    "$$\n",
    "\n",
    "where $ Z $ is the partition function. This formulation naturally leads to the **softmax function**.\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{M} a_j X_j = X_{\\text{total}}.\n",
    "$$\n",
    "\n",
    "To find the most probable configuration (i.e. the set $ \\{a_j\\} $) that maximizes the multiplicity $ W $, we maximize the logarithm of $ W $ (using Stirling’s approximation) subject to two constraints:\n",
    "- **Normalization:** $ \\sum_{j=1}^{M} a_j = N, $\n",
    "- **Logit Constraint:** $ \\sum_{j=1}^{M} a_j X_j = X_{\\text{total}}. $\n",
    "\n",
    "The corresponding Lagrangian is\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\ln W + \\alpha \\left( \\sum_{j=1}^{M} a_j - N \\right) + \\beta \\left( \\sum_{j=1}^{M} a_j X_j - X_{\\text{total}} \\right).\n",
    "$$\n",
    "\n",
    "Taking the derivative with respect to $ a_j $ and applying Stirling’s approximation (so that $ \\frac{\\partial \\ln W}{\\partial a_j} \\approx -\\ln a_j - 1 $), we have\n",
    "\n",
    "$$\n",
    "-\\ln a_j - 1 + \\alpha + \\beta X_j = 0.\n",
    "$$\n",
    "\n",
    "Rearranging yields\n",
    "\n",
    "$$\n",
    "\\ln a_j = \\alpha' + \\beta X_j,\n",
    "$$\n",
    "\n",
    "where $ \\alpha' = \\alpha - 1 $. Exponentiating both sides gives\n",
    "\n",
    "$$\n",
    "a_j = e^{\\alpha'} e^{\\beta X_j}.\n",
    "$$\n",
    "\n",
    "Normalization requires that\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{M} a_j = e^{\\alpha'} \\sum_{j=1}^{M} e^{\\beta X_j} = N,\n",
    "$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\n",
    "e^{\\alpha'} = \\frac{N}{\\sum_{j=1}^{M} e^{\\beta X_j}}.\n",
    "$$\n",
    "\n",
    "Substituting back, the count for each state becomes\n",
    "\n",
    "$$\n",
    "a_j = \\frac{N e^{\\beta X_j}}{\\sum_{k=1}^{M} e^{\\beta X_k}}.\n",
    "$$\n",
    "\n",
    "Thus, the probability for a data point to be assigned to class $ j $ is\n",
    "\n",
    "$$\n",
    "P(y = j) = \\frac{a_j}{N} = \\frac{e^{\\beta X_j}}{\\sum_{k=1}^{M} e^{\\beta X_k}}.\n",
    "$$\n",
    "\n",
    "If we absorb the Lagrange multiplier $ \\beta $ into the definition of the logit (or set $ \\beta = 1 $ by a suitable choice of units), this immediately recovers the familiar softmax function used in multiclass logistic regression:\n",
    "\n",
    "$$\n",
    "P(y = j | x) = \\frac{e^{X_j}}{\\sum_{k=1}^{M} e^{X_k}},\n",
    "$$\n",
    "\n",
    "with $ X_j = \\beta_j x $ (including any bias terms as needed).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
