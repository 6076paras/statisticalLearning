{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "\n",
    "\n",
    "Each data point $\\mathbf{x}_i$ is a vector $\\mathbf{x}_i \\in \\mathbb{R}^m$ consisting of $m$ features, with an associated output $y_i$. When considering multiple data points, we organize them into a feature matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times (m+1)}$, where $n$ is the number of data points, and an additional column of ones is included to account for the intercept term. Assuming that there is a linear line of best fit, we can use linear approximation to predict the y values, with given x vector.\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_mx_m+ \\epsilon $$\n",
    "\n",
    "For the second data point, we denote:\n",
    "\n",
    "$$ y_2 = \\beta_0 + \\beta_1 x_{21} + \\beta_2 x_{22} + \\dots + \\beta_mx_{2m} + \\epsilon_2 $$\n",
    "\n",
    "For multiple data points:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{First data point:} \\quad y_1 = \\beta_0 + \\beta_1 x_{11} + \\beta_2 x_{12} + \\dots + \\beta_mx_{1m} + \\epsilon_1 \\\\\n",
    "&\\text{Second data point:} \\quad y_2 = \\beta_0 + \\beta_1 x_{21} + \\beta_2 x_{22} + \\dots + \\beta_mx_{2m} + \\epsilon_2 \\\\\n",
    "&\\text{Third data point:} \\quad y_3 = \\beta_0 + \\beta_1 x_{31} + \\beta_2 x_{32} + \\dots + \\beta_mx_{3m} + \\epsilon_3\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "And so on...\n",
    "\n",
    "## Matrix Representation\n",
    "\n",
    "Organizing this into a matrix notation:\n",
    "\n",
    "$$ \\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} $$\n",
    "\n",
    "where:\n",
    "- $ \\mathbf{Y} $ is the vector of observed values,\n",
    "- $ \\mathbf{X} $ is the matrix of input features ($\\mathbf{X} \\in \\mathbb{R}^{n \\times (m+1)}$),\n",
    "- $ \\boldsymbol{\\beta} $ is the vector of coefficients ($\\boldsymbol{\\beta} \\in \\mathbb{R}^{(m+1)}$),\n",
    "- $ \\boldsymbol{\\epsilon} $ is the error term.\n",
    "\n",
    "## Error in Prediction\n",
    "\n",
    "For each data point:\n",
    "\n",
    "$$ e_i = y_i - \\hat{y}_i $$\n",
    "\n",
    "where $ \\hat{y}_i $ is the predicted value.\n",
    "\n",
    "## Residual Sum of Squares (RSS)\n",
    "\n",
    "The sum of squared errors (RSS) is given by:\n",
    "\n",
    "$$ RSS = \\sum (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "Inspecting RSS, we see that it is a function of $ \\boldsymbol{\\beta} $:\n",
    "\n",
    "$$ RSS = (\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta})^T (\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}) $$\n",
    "\n",
    "## Minimizing RSS\n",
    "\n",
    "To minimize RSS, we take the derivative:\n",
    "\n",
    "$$ \\frac{\\partial RSS}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}) = 0 $$\n",
    "\n",
    "Solving for $ \\boldsymbol{\\beta} $:\n",
    "\n",
    "$$ \\boldsymbol{\\beta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y} $$\n",
    "\n",
    "### Conditions for Existencei\n",
    "\n",
    "For $ (\\mathbf{X}^T \\mathbf{X})^{-1} $ to exist, $ \\mathbf{X}^T \\mathbf{X} $ must be invertible (full rank, linearly independent columns). If it is not invertible, we use:\n",
    "\n",
    "- **Regularization**: Adding a term to make it invertible\n",
    "- **Moore-Penrose Pseudo Inverse**:\n",
    "\n",
    "  $$ \\mathbf{X}^+ = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T $$\n",
    "\n",
    "This ensures a solution even if $ \\mathbf{X}^T \\mathbf{X} $ is not full rank.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
