{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "\n",
    "\n",
    "Each data point $x$ is a vector $\\mathbf{x} \\in \\mathbb{R}^m$ consisting of $m$ features, with an associated output $y$. Assuming that there is a linear line of best fit, we can use linear approximation to predict the y values, with given x vector. \n",
    "\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p + \\epsilon $$\n",
    "\n",
    "For the second data point, we denote:\n",
    "\n",
    "$$ y_2 = \\beta_0 + \\beta_1 x_{21} + \\beta_2 x_{22} + \\dots + \\beta_p x_{2p} + \\epsilon_2 $$\n",
    "\n",
    "For multiple data points:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{First data point:} \\quad y_1 = \\beta_0 + \\beta_1 x_{11} + \\beta_2 x_{12} + \\dots + \\beta_p x_{1p} + \\epsilon_1 \\\\\n",
    "&\\text{Second data point:} \\quad y_2 = \\beta_0 + \\beta_1 x_{21} + \\beta_2 x_{22} + \\dots + \\beta_p x_{2p} + \\epsilon_2 \\\\\n",
    "&\\text{Third data point:} \\quad y_3 = \\beta_0 + \\beta_1 x_{31} + \\beta_2 x_{32} + \\dots + \\beta_p x_{3p} + \\epsilon_3\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "And so on...\n",
    "\n",
    "## Matrix Representation\n",
    "\n",
    "Organizing this into a matrix notation:\n",
    "\n",
    "$$ Y = X \\beta + \\epsilon $$\n",
    "\n",
    "where:\n",
    "- $ Y $ is the vector of observed values,\n",
    "- $ X $ is the matrix of input features,\n",
    "- $ \\beta $ is the vector of coefficients,\n",
    "- $ \\epsilon $ is the error term.\n",
    "\n",
    "## Error in Prediction\n",
    "\n",
    "For each data point:\n",
    "\n",
    "$$ e_i = y_i - \\hat{y}_i $$\n",
    "\n",
    "where $ \\hat{y}_i $ is the predicted value.\n",
    "\n",
    "## Residual Sum of Squares (RSS)\n",
    "\n",
    "The sum of squared errors (RSS) is given by:\n",
    "\n",
    "$$ RSS = \\sum (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "Inspecting RSS, we see that it is a function of $ \\beta $:\n",
    "\n",
    "$$ RSS = (Y - X\\beta)^T (Y - X\\beta) $$\n",
    "\n",
    "## Minimizing RSS\n",
    "\n",
    "To minimize RSS, we take the derivative:\n",
    "\n",
    "$$ \\frac{\\partial RSS}{\\partial \\beta} = -2X^T(Y - X\\beta) = 0 $$\n",
    "\n",
    "Solving for $ \\beta $:\n",
    "\n",
    "$$ \\beta = (X^T X)^{-1} X^T Y $$\n",
    "\n",
    "### Conditions for Existence\n",
    "\n",
    "For $ (X^T X)^{-1} $ to exist, $ X^T X $ must be invertible (full rank, linearly independent columns). If it is not invertible, we use:\n",
    "\n",
    "- **Regularization**: Adding a term to make it invertible\n",
    "- **Moore-Penrose Pseudo Inverse**:\n",
    "\n",
    "  $$ X^+ = (X^T X)^{-1} X^T $$\n",
    "\n",
    "This ensures a solution even if $ X^T X $ is not full rank.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
